{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.function import InplaceFunction, Function\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, quantize = False, bits = 8, qmode = \"dynamic\"):\n",
    "    if quantize:\n",
    "        print(\"Quantize mode on\")\n",
    "        for layer in model.modules():\n",
    "            try:\n",
    "                mode = layer.mode()\n",
    "                if mode == False:\n",
    "                    layer.change_mod(True, bits, qmode)\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        print(\"Quantize mode off\")\n",
    "        for layer in model.modules():\n",
    "            try:\n",
    "                mode = layer.mode()\n",
    "                if mode == True:\n",
    "                    layer.change_mod(False, 0)\n",
    "            except:\n",
    "                continue\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qsin_activation_mode(model):\n",
    "    print(\"QSIN activation mode on\")\n",
    "    for layer in model.modules():\n",
    "        try:\n",
    "            mode = layer.mode()\n",
    "            layer.qsinmode()\n",
    "        except:\n",
    "            continue\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRound(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quantize_tensor(input_tensor, max_abs_val = None, num_bits = 8):\n",
    "    my_round = MyRound.apply\n",
    "    qmin = -1.0 * (2**num_bits) / 2\n",
    "    qmax = -qmin - 1\n",
    "    scale = max_abs_val / ((qmax - qmin) / 2)\n",
    "    input_tensor = torch.div(input_tensor, scale)\n",
    "    input_tensor = my_round((input_tensor))\n",
    "    input_tensor = torch.clamp(input_tensor, qmin, qmax)\n",
    "    return torch.mul(input_tensor, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quant(nn.Module):\n",
    "    def __init__(self, num_bits=8, mode = \"dynamic\", static_count = 30):\n",
    "        super(Quant, self).__init__()\n",
    "        self.num_bits = num_bits\n",
    "        self.mode = mode\n",
    "        self.static_count = static_count\n",
    "        self.static_cur = 0\n",
    "        self.stat_values = []\n",
    "        self.max_abs = 0 \n",
    "        if mode != \"dynamic\":\n",
    "            self.max_abs_tr = nn.Parameter(torch.zeros(0), requires_grad=True) # IMPORTANT\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if self.mode == \"dynamic\":\n",
    "            self.max_abs = torch.max(torch.abs(input.detach()))\n",
    "            return Quantize_tensor(input, self.max_abs, self.num_bits)\n",
    "        \n",
    "        elif self.mode == \"static\":\n",
    "            if self.static_cur > self.static_count:\n",
    "                return Quantize_tensor(input, self.max_abs_tr, self.num_bits)\n",
    "            elif self.static_cur == self.static_count:\n",
    "                self.max_abs = np.mean(self.stat_values)\n",
    "                self.max_abs_tr.data = torch.tensor(self.max_abs, dtype=torch.float).to(self.max_abs_tr.device)\n",
    "                self.static_cur += 1\n",
    "                return Quantize_tensor(input, self.max_abs_tr, self.num_bits)\n",
    "            else:\n",
    "                self.static_cur += 1\n",
    "                self.stat_values.append(np.max(np.absolute(input.cpu().detach().numpy())))\n",
    "                return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QSin(x, num_bits = 8): \n",
    "    pi = torch.tensor(np.pi)\n",
    "    qmin = -1.0 * (2**num_bits) / 2\n",
    "    qmax = -qmin - 1\n",
    "    result = torch.sum(torch.square(torch.sin(torch.mul(pi, x[torch.logical_and(x >= qmin, x <= qmax)]))))\n",
    "    result = result + torch.sum(torch.mul(torch.square(pi), torch.square((x[x < qmin] - qmin))))\n",
    "    result = result + torch.sum(torch.mul(torch.square(pi), torch.square((x[x > qmax] - qmax))))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, quantization: bool = False, q_bits: int = 8, qsin_activation = False):\n",
    "        super(Linear, self).__init__(in_features, out_features, bias)\n",
    "\n",
    "        self.quantize = True if quantization else False\n",
    "        self.QsinA = True if qsin_activation else False\n",
    "\n",
    "        if self.quantize:\n",
    "            self.bits = q_bits\n",
    "            self.Quantize_weights = Quant(self.bits)\n",
    "            self.Quantize_input = Quant(8) \n",
    "        else:\n",
    "            self.bits = 'FP'\n",
    "            \n",
    "        if self.QsinA:\n",
    "            self.qsin_loss_A = 0\n",
    "\n",
    "    def init(self, input):\n",
    "        self.inputW = input.shape\n",
    "        \n",
    "    def change_mod(self, value, bits = 8, mode = \"dynamic\"):\n",
    "        self.quantize = value\n",
    "        self.bits = bits\n",
    "        self.Quantize_weights = Quant(bits, mode)\n",
    "        self.Quantize_input = Quant(8, mode)\n",
    "        \n",
    "    def qsinmode(self):\n",
    "        self.QsinA = True\n",
    "        self.qsin_loss_A = 0\n",
    "        \n",
    "    def mode(self):\n",
    "        return self.quantize  \n",
    "\n",
    "    def forward(self, input):\n",
    "            \n",
    "        if self.quantize:\n",
    "            qinput = self.Quantize_input(input)\n",
    "            qweight = self.Quantize_weights(self.weight)\n",
    "            \n",
    "            #count qsin loss on activation\n",
    "            if self.QsinA:\n",
    "                self.qsin_loss_A = 0\n",
    "                qmin = -1.0 * (2**8) / 2\n",
    "                qmax = -qmin - 1\n",
    "                scale = self.Quantize_input.max_abs_tr / ((qmax - qmin) / 2)\n",
    "                sq_scale = torch.square(scale)\n",
    "                self.qsin_loss_A = torch.mul(sq_scale, QSin(torch.div(input, scale), 8))\n",
    "            \n",
    "            return nn.functional.linear(qinput, qweight, self.bias)\n",
    "        else:\n",
    "            return nn.functional.linear(input, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Embedding):\n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx=None, max_norm=None,\n",
    "                 norm_type=2.0, scale_grad_by_freq=False, sparse=False,\n",
    "                 quantization: bool = False, q_bits: int = 8):\n",
    "        super(Embedding, self).__init__(num_embeddings, embedding_dim, padding_idx)\n",
    "\n",
    "        self.quantize = True if quantization else False\n",
    "\n",
    "        if self.quantize:\n",
    "            self.bits = q_bits\n",
    "            self.Quantize_weights = Quant(self.bits)\n",
    "        else:\n",
    "            self.bits = 'FP'\n",
    "\n",
    "    def init(self, input):\n",
    "        self.inputW = input.shape\n",
    "        \n",
    "    def change_mod(self, value, bits = 8, mode = \"dynamic\"):\n",
    "        self.quantize = value\n",
    "        self.bits = bits\n",
    "        self.Quantize_weights = Quant(bits, mode)\n",
    "        \n",
    "    def mode(self):\n",
    "        return self.quantize  \n",
    "\n",
    "    def forward(self, input):\n",
    "            \n",
    "        if self.quantize:\n",
    "            qweight = self.Quantize_weights(self.weight)\n",
    "        \n",
    "            return nn.functional.embedding(input, qweight, self.padding_idx, self.max_norm,\n",
    "                 self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
    "        else:\n",
    "            return nn.functional.embedding(input, self.weight, self.padding_idx, self.max_norm,\n",
    "                 self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qsin_W(model, bits = 8):\n",
    "    qmin = -1.0 * (2**bits) / 2\n",
    "    qmax = -qmin - 1\n",
    "    loss = 0\n",
    "    for layer in model.modules():\n",
    "        try:\n",
    "            scale = layer.Quantize_weights.max_abs_tr / ((qmax - qmin) / 2)\n",
    "            sq_scale = torch.square(scale)\n",
    "            QSin_w = QSin(torch.div(layer.weight, scale), bits)\n",
    "            loss = loss + torch.mul(sq_scale, QSin_w)\n",
    "        except:\n",
    "            continue\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qsin_A(model):\n",
    "    loss = 0\n",
    "    for layer in model.modules():\n",
    "        try:\n",
    "            loss = loss + layer.qsin_loss_A\n",
    "        except:\n",
    "            continue\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_Linear(in_features, out_features, bias, weight):\n",
    "    linear = Linear(in_features, out_features)\n",
    "    linear.bias = bias\n",
    "    linear.weight = weight\n",
    "    return linear\n",
    "\n",
    "def get_custom_Embeding(num_embeddings, embedding_dim, padding_idx, weight):\n",
    "    embedding = Embedding(num_embeddings, embedding_dim, padding_idx)\n",
    "    embedding.weight = weight\n",
    "    return embedding\n",
    "\n",
    "def change_layers(model):\n",
    "    for name, layer in model.named_children():\n",
    "        #if name == 'intermediate' or \\\n",
    "        #name == 'output'or name == 'embeddings':\n",
    "        #   continue\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            setattr(model, name, get_custom_Linear(\n",
    "                                                layer.in_features,\n",
    "                                                layer.out_features,\n",
    "                                                layer.bias,\n",
    "                                                layer.weight\n",
    "            ))\n",
    "            \n",
    "        if isinstance(layer, nn.Embedding):\n",
    "            setattr(model, name, get_custom_Embeding(\n",
    "                                                layer.num_embeddings,\n",
    "                                                layer.embedding_dim,\n",
    "                                                layer.padding_idx,\n",
    "                                                layer.weight\n",
    "            ))\n",
    "        change_layers(getattr(model, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pooler\n",
    "#classifier\n",
    "#attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, quantize = False, bits = 8, qmode = \"dynamic\"):\n",
    "    if quantize:\n",
    "        print(\"Quantize mode on\")\n",
    "        for name, layer in model.named_modules():\n",
    "            try:\n",
    "                mode = layer.mode()\n",
    "                if mode == False:\n",
    "                    if 'pooler' in name or 'attention' in name or 'token_type_embeddings' in name:\n",
    "                        layer.change_mod(True, 4, qmode)\n",
    "                    elif 'classifier' in name: continue \n",
    "                    else: layer.change_mod(True, 8, qmode)\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        print(\"Quantize mode off\")\n",
    "        for layer in model.modules():\n",
    "            try:\n",
    "                mode = layer.mode()\n",
    "                if mode == True:\n",
    "                    layer.change_mod(False, 0)\n",
    "            except:\n",
    "                continue\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pooler\n",
    "#classifier\n",
    "#attention\n",
    "\n",
    "        #if name == 'intermediate' or \\\n",
    "        #name == 'output'or name == 'embeddings':\n",
    "        #    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQUAD metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SQuAD metric. \"\"\"\n",
    "\n",
    "import datasets\n",
    "\n",
    "from evaluate import evaluate\n",
    "\n",
    "\n",
    "_CITATION = \"\"\n",
    "\n",
    "_DESCRIPTION = \"\"\n",
    "\n",
    "_KWARGS_DESCRIPTION = \"\"\n",
    "\n",
    "@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\n",
    "class Squad(datasets.Metric):\n",
    "    def _info(self):\n",
    "        return datasets.MetricInfo(\n",
    "            description=_DESCRIPTION,\n",
    "            citation=_CITATION,\n",
    "            inputs_description=_KWARGS_DESCRIPTION,\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"predictions\": {\"id\": datasets.Value(\"string\"), \"prediction_text\": datasets.Value(\"string\")},\n",
    "                    \"references\": {\n",
    "                        \"id\": datasets.Value(\"string\"),\n",
    "                        \"answers\": datasets.features.Sequence(\n",
    "                            {\n",
    "                                \"text\": datasets.Value(\"string\"),\n",
    "                                \"answer_start\": datasets.Value(\"int32\"),\n",
    "                            }\n",
    "                        ),\n",
    "                    },\n",
    "                }\n",
    "            ),\n",
    "            codebase_urls=[\"https://rajpurkar.github.io/SQuAD-explorer/\"],\n",
    "            reference_urls=[\"https://rajpurkar.github.io/SQuAD-explorer/\"],\n",
    "        )\n",
    "\n",
    "    def _compute(self, predictions, references):\n",
    "        pred_dict = {prediction[\"id\"]: prediction[\"prediction_text\"] for prediction in predictions}\n",
    "        dataset = [\n",
    "            {\n",
    "                \"paragraphs\": [\n",
    "                    {\n",
    "                        \"qas\": [\n",
    "                            {\n",
    "                                \"answers\": [{\"text\": answer_text} for answer_text in ref[\"answers\"][\"text\"]],\n",
    "                                \"id\": ref[\"id\"],\n",
    "                            }\n",
    "                            for ref in references\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        score = evaluate(dataset=dataset, predictions=pred_dict)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "\n",
    "def eval_squad(trainer, validation_features, datasets, metric):\n",
    "\n",
    "    raw_predictions = trainer.predict(validation_features)\n",
    "    validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))\n",
    "    max_answer_length = 30\n",
    "\n",
    "    examples = datasets[\"validation\"]\n",
    "    features = validation_features\n",
    "\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "        all_start_logits, all_end_logits = raw_predictions\n",
    "        # Build a map example to its corresponding features.\n",
    "        example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "        features_per_example = collections.defaultdict(list)\n",
    "        for i, feature in enumerate(features):\n",
    "            features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "        # The dictionaries we have to fill.\n",
    "        predictions = collections.OrderedDict()\n",
    "\n",
    "        # Logging.\n",
    "        print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "        # Let's loop over all the examples!\n",
    "        for example_index, example in enumerate(tqdm(examples)):\n",
    "            # Those are the indices of the features associated to the current example.\n",
    "            feature_indices = features_per_example[example_index]\n",
    "\n",
    "            min_null_score = None # Only used if squad_v2 is True.\n",
    "            valid_answers = []\n",
    "\n",
    "            context = example[\"context\"]\n",
    "            # Looping through all the features associated to the current example.\n",
    "            for feature_index in feature_indices:\n",
    "                # We grab the predictions of the model for this feature.\n",
    "                start_logits = all_start_logits[feature_index]\n",
    "                end_logits = all_end_logits[feature_index]\n",
    "                # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "                # context.\n",
    "                offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "                # Update minimum null prediction.\n",
    "                cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "                feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "                if min_null_score is None or min_null_score < feature_null_score:\n",
    "                    min_null_score = feature_null_score\n",
    "\n",
    "                # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "                start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "                end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "                for start_index in start_indexes:\n",
    "                    for end_index in end_indexes:\n",
    "                        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                        # to part of the input_ids that are not in the context.\n",
    "                        if (\n",
    "                            start_index >= len(offset_mapping)\n",
    "                            or end_index >= len(offset_mapping)\n",
    "                            or offset_mapping[start_index] is None\n",
    "                            or offset_mapping[end_index] is None\n",
    "                        ):\n",
    "                            continue\n",
    "                        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                            continue\n",
    "\n",
    "                        start_char = offset_mapping[start_index][0]\n",
    "                        end_char = offset_mapping[end_index][1]\n",
    "                        valid_answers.append(\n",
    "                            {\n",
    "                                \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                                \"text\": context[start_char: end_char]\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            if len(valid_answers) > 0:\n",
    "                best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "            else:\n",
    "                # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "                # failure.\n",
    "                best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "    print(metric.compute(predictions=formatted_predictions, references=references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('./models/squad/model-bert-base/', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('./models/tokenizer-bert-base/', local_files_only=True)\n",
    "\n",
    "batch_size = 16\n",
    "max_length = 384\n",
    "doc_stride = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "encoded_dataset = load_from_disk('cur_squad_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "args = TrainingArguments(\n",
    "    f\"test-squad\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = encoded_dataset = load_from_disk('cur_squad_val_data')\n",
    "metric = Squad()\n",
    "datasets = load_from_disk(\"cur_squad_set_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_squad(trainer, validation_features, datasets, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('./test-squad/checkpoint-8000/', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "encoded_dataset = load_from_disk('cur_squad_data')\n",
    "args = TrainingArguments(\n",
    "    f\"test-squad\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_squad(trainer, validation_features, datasets, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = quantize_model(model, quantize=False, bits = 4)\n",
    "model = quantize_model(model, quantize=True, bits = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = load_from_disk('cur_squad_data')\n",
    "trainer_DQ = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_squad(trainer_DQ, validation_features, datasets, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = quantize_model(model, quantize=False, bits = 4)\n",
    "model = quantize_model(model, quantize=True, bits = 4, qmode = \"static\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = load_from_disk('cur_squad_data')\n",
    "train_enc = encoded_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_enc.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])\n",
    "train_loader = torch.utils.data.DataLoader(train_enc, batch_size=8, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "i = 0\n",
    "for batch in tqdm(train_loader):\n",
    "    i += 1\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    start_positions = batch['start_positions'].to(device)\n",
    "    end_positions = batch['end_positions'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "    if i == 32:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = load_from_disk('cur_squad_data')\n",
    "trainer_SQ = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_squad(trainer_SQ, validation_features, datasets, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QSIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qsin_activation_mode(model)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "class QSinTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        Qsin_W_loss = Qsin_W(model, 4)\n",
    "        Qsin_A_loss = Qsin_A(model)\n",
    "        L = outputs[0]\n",
    "        lambda_w = 10 ** (np.round(np.log10(Qsin_W_loss.cuda().tolist()) - np.log10(L.cuda().tolist())))\n",
    "        lambda_a = 10 ** (np.round(np.log10(Qsin_A_loss.cuda().tolist()) - np.log10(L.cuda().tolist()))+1)\n",
    "        loss = L + Qsin_W_loss / lambda_w + Qsin_A_loss / lambda_a\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = encoded_dataset = load_from_disk('cur_squad_val_data')\n",
    "metric = Squad()\n",
    "datasets = load_from_disk(\"cur_squad_set_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "encoded_dataset = load_from_disk('cur_squad_data')\n",
    "args = TrainingArguments(\n",
    "    \"qsin_train_tmp\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=32,\n",
    "    #eval_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer_QSin = QSinTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset['validation'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class MyCallback(TrainerCallback):\n",
    "    \"A callback that prints a message at the beginning of training\"\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        print(\"Starting evaluate:\")\n",
    "        cur_tr = QSinTrainer(model,args,train_dataset=encoded_dataset[\"train\"],\n",
    "                             eval_dataset=encoded_dataset['validation'],tokenizer=tokenizer)\n",
    "        eval_squad(cur_tr, validation_features, datasets, metric)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_QSin.add_callback(MyCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_QSin.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_squad(trainer_QSin, validation_features, datasets, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "args = TrainingArguments(\n",
    "    \"qat_train_tmp\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=32,\n",
    "    #eval_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer_qat = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset['validation'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class MyCallback(TrainerCallback):\n",
    "    \"A callback that prints a message at the beginning of training\"\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        print(\"Starting evaluate:\")\n",
    "        cur_tr = Trainer(model,args,train_dataset=encoded_dataset[\"train\"],\n",
    "                             eval_dataset=encoded_dataset['validation'],tokenizer=tokenizer)\n",
    "        eval_squad(cur_tr, validation_features, datasets, metric)\n",
    "        \n",
    "trainer_qat.add_callback(MyCallback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_qat.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = encoded_dataset = load_from_disk('cur_squad_val_data')\n",
    "metric = Squad()\n",
    "datasets = load_from_disk(\"cur_squad_set_data/\")\n",
    "eval_squad(trainer_qat, validation_features, datasets, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
