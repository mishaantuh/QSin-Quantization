{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.function import InplaceFunction, Function\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, quantize = False, bits = 8, qmode = \"dynamic\"):\n",
    "    if quantize:\n",
    "        print(\"Quantize mode on\")\n",
    "        for layer in model.modules():\n",
    "            try:\n",
    "                mode = layer.mode()\n",
    "                if mode == False:\n",
    "                    layer.change_mod(True, bits, qmode)\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        print(\"Quantize mode off\")\n",
    "        for layer in model.modules():\n",
    "            try:\n",
    "                mode = layer.mode()\n",
    "                if mode == True:\n",
    "                    layer.change_mod(False, 0)\n",
    "            except:\n",
    "                continue\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qsin_activation_mode(model):\n",
    "    print(\"QSIN activation mode on\")\n",
    "    for layer in model.modules():\n",
    "        try:\n",
    "            mode = layer.mode()\n",
    "            layer.qsinmode()\n",
    "        except:\n",
    "            continue\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRound(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quantize_tensor(input_tensor, max_abs_val = None, num_bits = 8):\n",
    "    my_round = MyRound.apply\n",
    "    qmin = -1.0 * (2**num_bits) / 2\n",
    "    qmax = -qmin - 1\n",
    "    scale = max_abs_val / ((qmax - qmin) / 2)\n",
    "    input_tensor = torch.div(input_tensor, scale)\n",
    "    input_tensor = my_round((input_tensor))\n",
    "    input_tensor = torch.clamp(input_tensor, qmin, qmax)\n",
    "    return torch.mul(input_tensor, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quant(nn.Module):\n",
    "    def __init__(self, num_bits=8, mode = \"dynamic\", static_count = 30):\n",
    "        super(Quant, self).__init__()\n",
    "        self.num_bits = num_bits\n",
    "        self.mode = mode\n",
    "        self.static_count = static_count\n",
    "        self.static_cur = 0\n",
    "        self.stat_values = []\n",
    "        self.max_abs = 0 \n",
    "        if mode != \"dynamic\":\n",
    "            self.max_abs_tr = nn.Parameter(torch.zeros(0), requires_grad=True) # IMPORTANT\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if self.mode == \"dynamic\":\n",
    "            self.max_abs = torch.max(torch.abs(input.detach()))\n",
    "            return Quantize_tensor(input, self.max_abs, self.num_bits)\n",
    "        \n",
    "        elif self.mode == \"static\":\n",
    "            if self.static_cur > self.static_count:\n",
    "                return Quantize_tensor(input, self.max_abs_tr, self.num_bits)\n",
    "            elif self.static_cur == self.static_count:\n",
    "                self.max_abs = np.mean(self.stat_values)\n",
    "                self.max_abs_tr.data = torch.tensor(self.max_abs, dtype=torch.float).to(self.max_abs_tr.device)\n",
    "                self.static_cur += 1\n",
    "                return Quantize_tensor(input, self.max_abs_tr, self.num_bits)\n",
    "            else:\n",
    "                self.static_cur += 1\n",
    "                self.stat_values.append(np.max(np.absolute(input.cpu().detach().numpy())))\n",
    "                return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QSin(x, num_bits = 8): \n",
    "    pi = torch.tensor(np.pi)\n",
    "    qmin = -1.0 * (2**num_bits) / 2\n",
    "    qmax = -qmin - 1\n",
    "    result = torch.sum(torch.square(torch.sin(torch.mul(pi, x[torch.logical_and(x >= qmin, x <= qmax)]))))\n",
    "    result = result + torch.sum(torch.mul(torch.square(pi), torch.square((x[x < qmin] - qmin))))\n",
    "    result = result + torch.sum(torch.mul(torch.square(pi), torch.square((x[x > qmax] - qmax))))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(nn.Conv2d):\n",
    "    def __init__(self, inCh: int, outCh: int, kernel_size: int = 4, stride: int = 1, padding: int = 0,\n",
    "                 bias: bool = None, quantization: bool = False, q_bits: int = 8, qsin_activation = False):\n",
    "        super(Conv2d,self).__init__(inCh, outCh, kernel_size, stride=stride, padding = padding, bias = bias)\n",
    "\n",
    "        self.quantize = True if quantization else False\n",
    "        self.QsinA = True if qsin_activation else False\n",
    "\n",
    "        if self.quantize:\n",
    "            self.bits = q_bits\n",
    "            self.Quantize_weights = Quant(self.bits)\n",
    "            self.Quantize_input = Quant(self.bits)\n",
    "        else:\n",
    "            self.bits = 'FP'\n",
    "            \n",
    "        if self.QsinA:\n",
    "            self.qsin_loss_A = 0\n",
    "\n",
    "    def init(self, input):\n",
    "        self.inputW = input.shape\n",
    "\n",
    "    def change_mod(self, value, bits = 8, mode = \"dynamic\"):\n",
    "        self.quantize = value\n",
    "        self.bits = bits\n",
    "        self.Quantize_weights = Quant(bits,mode)\n",
    "        self.Quantize_input = Quant(bits, mode)\n",
    "    \n",
    "    def qsinmode(self):\n",
    "        self.QsinA = True\n",
    "        self.qsin_loss_A = 0\n",
    "\n",
    "    def mode(self):\n",
    "        return self.quantize  \n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.quantize:\n",
    "            qinput = self.Quantize_input(input)\n",
    "            qweight = self.Quantize_weights(self.weight)\n",
    "            \n",
    "            \n",
    "            #count qsin loss on activation\n",
    "            if self.QsinA:\n",
    "                self.qsin_loss_A = 0\n",
    "                qmin = -1.0 * (2**self.bits) / 2\n",
    "                qmax = -qmin - 1\n",
    "                scale = self.Quantize_input.max_abs_tr / ((qmax - qmin) / 2)\n",
    "                sq_scale = torch.square(scale)\n",
    "                self.qsin_loss_A = torch.mul(sq_scale, QSin(torch.div(input, scale), self.bits))\n",
    "                \n",
    "                \n",
    "            return nn.functional.conv2d(qinput, qweight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "        else:\n",
    "            return nn.functional.conv2d(input, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, quantization: bool = False, q_bits: int = 8, qsin_activation = False):\n",
    "        super(Linear, self).__init__(in_features, out_features, bias)\n",
    "\n",
    "        self.quantize = True if quantization else False\n",
    "        self.QsinA = True if qsin_activation else False\n",
    "\n",
    "        if self.quantize:\n",
    "            self.bits = q_bits\n",
    "            self.Quantize_weights = Quant(self.bits)\n",
    "            self.Quantize_input = Quant(self.bits)\n",
    "        else:\n",
    "            self.bits = 'FP'\n",
    "            \n",
    "        if self.QsinA:\n",
    "            self.qsin_loss_A = 0\n",
    "\n",
    "    def init(self, input):\n",
    "        self.inputW = input.shape\n",
    "        \n",
    "    def change_mod(self, value, bits = 8, mode = \"dynamic\"):\n",
    "        self.quantize = value\n",
    "        self.bits = bits\n",
    "        self.Quantize_weights = Quant(bits, mode)\n",
    "        self.Quantize_input = Quant(bits, mode)\n",
    "        \n",
    "    def qsinmode(self):\n",
    "        self.QsinA = True\n",
    "        self.qsin_loss_A = 0\n",
    "        \n",
    "    def mode(self):\n",
    "        return self.quantize  \n",
    "\n",
    "    def forward(self, input):\n",
    "            \n",
    "        if self.quantize:\n",
    "            qinput = self.Quantize_input(input)\n",
    "            qweight = self.Quantize_weights(self.weight)\n",
    "            \n",
    "            #count qsin loss on activation\n",
    "            if self.QsinA:\n",
    "                self.qsin_loss_A = 0\n",
    "                qmin = -1.0 * (2**self.bits) / 2\n",
    "                qmax = -qmin - 1\n",
    "                scale = self.Quantize_input.max_abs_tr / ((qmax - qmin) / 2)\n",
    "                sq_scale = torch.square(scale)\n",
    "                self.qsin_loss_A = torch.mul(sq_scale, QSin(torch.div(input, scale), self.bits))\n",
    "            \n",
    "            return nn.functional.linear(qinput, qweight, self.bias)\n",
    "        else:\n",
    "            return nn.functional.linear(input, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Embedding):\n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx=None, max_norm=None,\n",
    "                 norm_type=2.0, scale_grad_by_freq=False, sparse=False,\n",
    "                 quantization: bool = False, q_bits: int = 8):\n",
    "        super(Embedding, self).__init__(num_embeddings, embedding_dim, padding_idx)\n",
    "\n",
    "        self.quantize = True if quantization else False\n",
    "\n",
    "        if self.quantize:\n",
    "            self.bits = q_bits\n",
    "            self.Quantize_weights = Quant(self.bits)\n",
    "            self.Quantize_input = Quant(self.bits)\n",
    "        else:\n",
    "            self.bits = 'FP'\n",
    "\n",
    "    def init(self, input):\n",
    "        self.inputW = input.shape\n",
    "        \n",
    "    def change_mod(self, value, bits = 8, mode = \"dynamic\"):\n",
    "        self.quantize = value\n",
    "        self.bits = bits\n",
    "        self.Quantize_weights = Quant(bits, mode)\n",
    "        self.Quantize_input = Quant(bits, mode)\n",
    "        \n",
    "    def mode(self):\n",
    "        return self.quantize  \n",
    "\n",
    "    def forward(self, input):\n",
    "            \n",
    "        if self.quantize:\n",
    "            qweight = self.Quantize_weights(self.weight)\n",
    "        \n",
    "            return nn.functional.embedding(input, qweight, self.padding_idx, self.max_norm,\n",
    "                 self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
    "        else:\n",
    "            return nn.functional.embedding(input, self.weight, self.padding_idx, self.max_norm,\n",
    "                 self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qsin_W(model, bits = 8):\n",
    "    qmin = -1.0 * (2**bits) / 2\n",
    "    qmax = -qmin - 1\n",
    "    loss = 0\n",
    "    for layer in model.modules():\n",
    "        try:\n",
    "            scale = layer.Quantize_weights.max_abs_tr / ((qmax - qmin) / 2)\n",
    "            sq_scale = torch.square(scale)\n",
    "            QSin_w = QSin(torch.div(layer.weight, scale), bits)\n",
    "            loss = loss + torch.mul(sq_scale, QSin_w)\n",
    "        except:\n",
    "            continue\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qsin_A(model):\n",
    "    loss = 0\n",
    "    for layer in model.modules():\n",
    "        try:\n",
    "            loss = loss + layer.qsin_loss_A\n",
    "        except:\n",
    "            continue\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_Linear(in_features, out_features, bias, weight):\n",
    "    linear = Linear(in_features, out_features)\n",
    "    linear.bias = bias\n",
    "    linear.weight = weight\n",
    "    return linear\n",
    "\n",
    "def get_custom_Embeding(num_embeddings, embedding_dim, padding_idx, weight):\n",
    "    embedding = Embedding(num_embeddings, embedding_dim, padding_idx)\n",
    "    embedding.weight = weight\n",
    "    return embedding\n",
    "\n",
    "def change_layers(model, head = True):\n",
    "    for name, layer in model.named_children():\n",
    "        if head == False:\n",
    "            if name == 'classifier':\n",
    "                continue\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            setattr(model, name, get_custom_Linear(\n",
    "                                                layer.in_features,\n",
    "                                                layer.out_features,\n",
    "                                                layer.bias,\n",
    "                                                layer.weight\n",
    "            ))\n",
    "            \n",
    "        if isinstance(layer, nn.Embedding):\n",
    "            setattr(model, name, get_custom_Embeding(\n",
    "                                                layer.num_embeddings,\n",
    "                                                layer.embedding_dim,\n",
    "                                                layer.padding_idx,\n",
    "                                                layer.weight\n",
    "            ))\n",
    "        change_layers(getattr(model, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# squad metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SQuAD metric. \"\"\"\n",
    "\n",
    "import datasets\n",
    "\n",
    "from evaluate import evaluate\n",
    "\n",
    "\n",
    "_CITATION = \"\"\n",
    "\n",
    "_DESCRIPTION = \"\"\n",
    "\n",
    "_KWARGS_DESCRIPTION = \"\"\n",
    "\n",
    "@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\n",
    "class Squad(datasets.Metric):\n",
    "    def _info(self):\n",
    "        return datasets.MetricInfo(\n",
    "            description=_DESCRIPTION,\n",
    "            citation=_CITATION,\n",
    "            inputs_description=_KWARGS_DESCRIPTION,\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"predictions\": {\"id\": datasets.Value(\"string\"), \"prediction_text\": datasets.Value(\"string\")},\n",
    "                    \"references\": {\n",
    "                        \"id\": datasets.Value(\"string\"),\n",
    "                        \"answers\": datasets.features.Sequence(\n",
    "                            {\n",
    "                                \"text\": datasets.Value(\"string\"),\n",
    "                                \"answer_start\": datasets.Value(\"int32\"),\n",
    "                            }\n",
    "                        ),\n",
    "                    },\n",
    "                }\n",
    "            ),\n",
    "            codebase_urls=[\"https://rajpurkar.github.io/SQuAD-explorer/\"],\n",
    "            reference_urls=[\"https://rajpurkar.github.io/SQuAD-explorer/\"],\n",
    "        )\n",
    "\n",
    "    def _compute(self, predictions, references):\n",
    "        pred_dict = {prediction[\"id\"]: prediction[\"prediction_text\"] for prediction in predictions}\n",
    "        dataset = [\n",
    "            {\n",
    "                \"paragraphs\": [\n",
    "                    {\n",
    "                        \"qas\": [\n",
    "                            {\n",
    "                                \"answers\": [{\"text\": answer_text} for answer_text in ref[\"answers\"][\"text\"]],\n",
    "                                \"id\": ref[\"id\"],\n",
    "                            }\n",
    "                            for ref in references\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        score = evaluate(dataset=dataset, predictions=pred_dict)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "\n",
    "def eval_squad(trainer, validation_features, datasets, metric):\n",
    "\n",
    "    raw_predictions = trainer.predict(validation_features)\n",
    "    validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))\n",
    "    max_answer_length = 30\n",
    "\n",
    "    examples = datasets[\"validation\"]\n",
    "    features = validation_features\n",
    "\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "        all_start_logits, all_end_logits = raw_predictions\n",
    "        # Build a map example to its corresponding features.\n",
    "        example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "        features_per_example = collections.defaultdict(list)\n",
    "        for i, feature in enumerate(features):\n",
    "            features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "        # The dictionaries we have to fill.\n",
    "        predictions = collections.OrderedDict()\n",
    "\n",
    "        # Logging.\n",
    "        print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "        # Let's loop over all the examples!\n",
    "        for example_index, example in enumerate(tqdm(examples)):\n",
    "            # Those are the indices of the features associated to the current example.\n",
    "            feature_indices = features_per_example[example_index]\n",
    "\n",
    "            min_null_score = None # Only used if squad_v2 is True.\n",
    "            valid_answers = []\n",
    "\n",
    "            context = example[\"context\"]\n",
    "            # Looping through all the features associated to the current example.\n",
    "            for feature_index in feature_indices:\n",
    "                # We grab the predictions of the model for this feature.\n",
    "                start_logits = all_start_logits[feature_index]\n",
    "                end_logits = all_end_logits[feature_index]\n",
    "                # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "                # context.\n",
    "                offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "                # Update minimum null prediction.\n",
    "                cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "                feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "                if min_null_score is None or min_null_score < feature_null_score:\n",
    "                    min_null_score = feature_null_score\n",
    "\n",
    "                # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "                start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "                end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "                for start_index in start_indexes:\n",
    "                    for end_index in end_indexes:\n",
    "                        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                        # to part of the input_ids that are not in the context.\n",
    "                        if (\n",
    "                            start_index >= len(offset_mapping)\n",
    "                            or end_index >= len(offset_mapping)\n",
    "                            or offset_mapping[start_index] is None\n",
    "                            or offset_mapping[end_index] is None\n",
    "                        ):\n",
    "                            continue\n",
    "                        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                            continue\n",
    "\n",
    "                        start_char = offset_mapping[start_index][0]\n",
    "                        end_char = offset_mapping[end_index][1]\n",
    "                        valid_answers.append(\n",
    "                            {\n",
    "                                \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                                \"text\": context[start_char: end_char]\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            if len(valid_answers) > 0:\n",
    "                best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "            else:\n",
    "                # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "                # failure.\n",
    "                best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "    print(metric.compute(predictions=formatted_predictions, references=references))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('./models/squad/model-bert-base/', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('./models/tokenizer-bert-base/', local_files_only=True)\n",
    "\n",
    "batch_size = 16\n",
    "max_length = 384\n",
    "doc_stride = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "encoded_dataset = load_from_disk('cur_squad_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "args = TrainingArguments(\n",
    "    f\"test-squad\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='8301' max='8301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8301/8301 1:48:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.082700</td>\n",
       "      <td>1.013363</td>\n",
       "      <td>80.278700</td>\n",
       "      <td>134.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>1.019003</td>\n",
       "      <td>80.512300</td>\n",
       "      <td>133.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.443400</td>\n",
       "      <td>1.153888</td>\n",
       "      <td>80.125000</td>\n",
       "      <td>134.590000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8301, training_loss=0.8238293668790375, metrics={'train_runtime': 6494.0765, 'train_samples_per_second': 1.278, 'total_flos': 6.662933266727117e+16, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = encoded_dataset = load_from_disk('cur_squad_val_data')\n",
    "metric = Squad()\n",
    "datasets = load_from_disk(\"cur_squad_set_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='337' max='337' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [337/337 01:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 10784 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8f1533d3f6481f87fa23f4dfd8934c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'exact_match': 80.54872280037843, 'f1': 88.18045550126546}\n"
     ]
    }
   ],
   "source": [
    "eval_squad(trainer, validation_features, datasets, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('./test-squad/checkpoint-8000/', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "encoded_dataset = load_from_disk('cur_squad_data')\n",
    "args = TrainingArguments(\n",
    "    f\"test-squad\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='337' max='337' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [337/337 01:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 10784 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e281f80517b40c2ac41edd67f4f6021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'exact_match': 80.6244087038789, 'f1': 88.22187065566567}\n"
     ]
    }
   ],
   "source": [
    "eval_squad(trainer, validation_features, datasets, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantize mode off\n",
      "Quantize mode on\n"
     ]
    }
   ],
   "source": [
    "model = quantize_model(model, quantize=False, bits = 8)\n",
    "model = quantize_model(model, quantize=True, bits = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = load_from_disk('cur_squad_data')\n",
    "trainer_DQ = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='337' max='337' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [337/337 01:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 10784 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887bbcd3425f4c909b877a0c2c08c9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'exact_match': 74.69252601702932, 'f1': 83.85539954522399}\n"
     ]
    }
   ],
   "source": [
    "eval_squad(trainer_DQ, validation_features, datasets, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantize mode off\n",
      "Quantize mode on\n"
     ]
    }
   ],
   "source": [
    "model = quantize_model(model, quantize=False, bits = 8)\n",
    "model = quantize_model(model, quantize=True, bits = 8, qmode = \"static\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = load_from_disk('cur_squad_data')\n",
    "train_enc = encoded_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_enc.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])\n",
    "train_loader = torch.utils.data.DataLoader(train_enc, batch_size=8, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2501f577b4b4a07b507876adc1c8b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11066.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "i = 0\n",
    "for batch in tqdm(train_loader):\n",
    "    i += 1\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    start_positions = batch['start_positions'].to(device)\n",
    "    end_positions = batch['end_positions'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "    if i == 32:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = load_from_disk('cur_squad_data')\n",
    "trainer_SQ = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='337' max='337' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [337/337 01:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 10784 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0758b8bab2949f69200670323b89868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'exact_match': 74.56953642384106, 'f1': 83.67325646566213}\n"
     ]
    }
   ],
   "source": [
    "eval_squad(trainer_SQ, validation_features, datasets, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QSIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QSIN activation mode on\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qsin_activation_mode(model)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "class QSinTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        Qsin_W_loss = Qsin_W(model, 8)\n",
    "        Qsin_A_loss = Qsin_A(model)\n",
    "        L = outputs[0]\n",
    "        lambda_w = 10 ** (np.round(np.log10(Qsin_W_loss.cuda().tolist()) - np.log10(L.cuda().tolist()))+1)\n",
    "        lambda_a = 10 ** (np.round(np.log10(Qsin_A_loss.cuda().tolist()) - np.log10(L.cuda().tolist()))+1)\n",
    "        loss = L + Qsin_W_loss / lambda_w + Qsin_A_loss / lambda_a\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "args = TrainingArguments(\n",
    "    \"qsin_train_tmp\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=32,\n",
    "    #eval_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer_QSin = QSinTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset['validation'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='22131' max='22131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22131/22131 10:51:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.940500</td>\n",
       "      <td>1.574125</td>\n",
       "      <td>200.077600</td>\n",
       "      <td>53.899000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.652300</td>\n",
       "      <td>1.684495</td>\n",
       "      <td>208.920200</td>\n",
       "      <td>51.618000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.385400</td>\n",
       "      <td>1.937070</td>\n",
       "      <td>201.574700</td>\n",
       "      <td>53.499000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at qsin_train_tmp/checkpoint-7377 were not used when initializing BertForQuestionAnswering: ['bert.embeddings.word_embeddings.Quantize_weights.max_abs_tr', 'bert.embeddings.word_embeddings.Quantize_input.max_abs_tr', 'bert.embeddings.position_embeddings.Quantize_weights.max_abs_tr', 'bert.embeddings.position_embeddings.Quantize_input.max_abs_tr', 'bert.embeddings.token_type_embeddings.Quantize_weights.max_abs_tr', 'bert.embeddings.token_type_embeddings.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.output.dense.Quantize_input.max_abs_tr', 'qa_outputs.Quantize_weights.max_abs_tr', 'qa_outputs.Quantize_input.max_abs_tr']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22131, training_loss=0.6590308974144834, metrics={'train_runtime': 39096.3708, 'train_samples_per_second': 0.566, 'total_flos': 6.662942383707648e+16, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_QSin.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='337' max='337' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [337/337 01:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 10784 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f3f0ef38934e83927b7394244e0118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'exact_match': 78.57142857142857, 'f1': 86.15204416291014}\n"
     ]
    }
   ],
   "source": [
    "validation_features = encoded_dataset = load_from_disk('cur_squad_val_data')\n",
    "metric = Squad()\n",
    "datasets = load_from_disk(\"cur_squad_set_data/\")\n",
    "eval_squad(trainer_QSin, validation_features, datasets, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "args = TrainingArguments(\n",
    "    \"qat_train_tmp\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=32,\n",
    "    #eval_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer_qat = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset['validation'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='22131' max='22131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22131/22131 2:24:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.669400</td>\n",
       "      <td>1.167522</td>\n",
       "      <td>80.950200</td>\n",
       "      <td>133.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.434400</td>\n",
       "      <td>1.352659</td>\n",
       "      <td>80.681600</td>\n",
       "      <td>133.661000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.221100</td>\n",
       "      <td>1.757163</td>\n",
       "      <td>80.630600</td>\n",
       "      <td>133.746000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22131, training_loss=0.4373031993009465, metrics={'train_runtime': 8691.3495, 'train_samples_per_second': 2.546, 'total_flos': 6.662933266727117e+16, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_qat.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='337' max='337' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [337/337 01:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 10784 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59ab7f639154a3bb83266d10be22180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'exact_match': 78.35383159886472, 'f1': 86.84785027142007}\n"
     ]
    }
   ],
   "source": [
    "validation_features = encoded_dataset = load_from_disk('cur_squad_val_data')\n",
    "metric = Squad()\n",
    "datasets = load_from_disk(\"cur_squad_set_data/\")\n",
    "eval_squad(trainer_qat, validation_features, datasets, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
