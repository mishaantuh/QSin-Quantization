{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241a537c",
   "metadata": {},
   "source": [
    "# Quantization modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "746e8636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.function import InplaceFunction, Function\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d13612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, quantize = False, bits = 8, qmode = \"dynamic\"):\n",
    "    if quantize:\n",
    "        print(\"Quantize mode on\")\n",
    "        for layer in model.modules():\n",
    "            try:\n",
    "                mode = layer.mode()\n",
    "                if mode == False:\n",
    "                    layer.change_mod(True, bits, qmode)\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        print(\"Quantize mode off\")\n",
    "        for layer in model.modules():\n",
    "            try:\n",
    "                mode = layer.mode()\n",
    "                if mode == True:\n",
    "                    layer.change_mod(False, 0)\n",
    "            except:\n",
    "                continue\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21efe560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qsin_activation_mode(model):\n",
    "    print(\"QSIN activation mode on\")\n",
    "    for layer in model.modules():\n",
    "        try:\n",
    "            mode = layer.mode()\n",
    "            layer.qsinmode()\n",
    "        except:\n",
    "            continue\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02c860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRound(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2fba68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quantize_tensor(input_tensor, max_abs_val = None, num_bits = 8):\n",
    "    my_round = MyRound.apply\n",
    "    qmin = -1.0 * (2**num_bits) / 2\n",
    "    qmax = -qmin - 1\n",
    "    scale = max_abs_val / ((qmax - qmin) / 2)\n",
    "    input_tensor = torch.div(input_tensor, scale)\n",
    "    input_tensor = my_round((input_tensor))\n",
    "    input_tensor = torch.clamp(input_tensor, qmin, qmax)\n",
    "    return torch.mul(input_tensor, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c91d7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quant(nn.Module):\n",
    "    def __init__(self, num_bits=8, mode = \"dynamic\", static_count = 30):\n",
    "        super(Quant, self).__init__()\n",
    "        self.num_bits = num_bits\n",
    "        self.mode = mode\n",
    "        self.static_count = static_count\n",
    "        self.static_cur = 0\n",
    "        self.stat_values = []\n",
    "        self.max_abs = 0 \n",
    "        if mode != \"dynamic\":\n",
    "            self.max_abs_tr = nn.Parameter(torch.zeros(0), requires_grad=True) # IMPORTANT\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if self.mode == \"dynamic\":\n",
    "            self.max_abs = torch.max(torch.abs(input.detach()))\n",
    "            return Quantize_tensor(input, self.max_abs, self.num_bits)\n",
    "        \n",
    "        elif self.mode == \"static\":\n",
    "            if self.static_cur > self.static_count:\n",
    "                return Quantize_tensor(input, self.max_abs_tr, self.num_bits)\n",
    "            elif self.static_cur == self.static_count:\n",
    "                self.max_abs = np.mean(self.stat_values)\n",
    "                self.max_abs_tr.data = torch.tensor(self.max_abs, dtype=torch.float).to(self.max_abs_tr.device)\n",
    "                self.static_cur += 1\n",
    "                return Quantize_tensor(input, self.max_abs_tr, self.num_bits)\n",
    "            else:\n",
    "                self.static_cur += 1\n",
    "                self.stat_values.append(np.max(np.absolute(input.cpu().detach().numpy())))\n",
    "                return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98dc035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QSin(x, num_bits = 8): \n",
    "    pi = torch.tensor(np.pi)\n",
    "    qmin = -1.0 * (2**num_bits) / 2\n",
    "    qmax = -qmin - 1\n",
    "    result = torch.sum(torch.square(torch.sin(torch.mul(pi, x[torch.logical_and(x >= qmin, x <= qmax)]))))\n",
    "    result = result + torch.sum(torch.mul(torch.square(pi), torch.square((x[x < qmin] - qmin))))\n",
    "    result = result + torch.sum(torch.mul(torch.square(pi), torch.square((x[x > qmax] - qmax))))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18bed00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, quantization: bool = False, q_bits: int = 8, qsin_activation = False):\n",
    "        super(Linear, self).__init__(in_features, out_features, bias)\n",
    "\n",
    "        self.quantize = True if quantization else False\n",
    "        self.QsinA = True if qsin_activation else False\n",
    "\n",
    "        if self.quantize:\n",
    "            self.bits = q_bits\n",
    "            self.Quantize_weights = Quant(self.bits)\n",
    "            self.Quantize_input = Quant(self.bits)\n",
    "        else:\n",
    "            self.bits = 'FP'\n",
    "            \n",
    "        if self.QsinA:\n",
    "            self.qsin_loss_A = 0\n",
    "\n",
    "    def init(self, input):\n",
    "        self.inputW = input.shape\n",
    "        \n",
    "    def change_mod(self, value, bits = 8, mode = \"dynamic\"):\n",
    "        self.quantize = value\n",
    "        self.bits = bits\n",
    "        self.Quantize_weights = Quant(bits, mode)\n",
    "        self.Quantize_input = Quant(bits, mode)\n",
    "        \n",
    "    def qsinmode(self):\n",
    "        self.QsinA = True\n",
    "        self.qsin_loss_A = 0\n",
    "        \n",
    "    def mode(self):\n",
    "        return self.quantize  \n",
    "\n",
    "    def forward(self, input):\n",
    "            \n",
    "        if self.quantize:\n",
    "            qinput = self.Quantize_input(input)\n",
    "            qweight = self.Quantize_weights(self.weight)\n",
    "            \n",
    "            #count qsin loss on activation\n",
    "            if self.QsinA:\n",
    "                self.qsin_loss_A = 0\n",
    "                qmin = -1.0 * (2**self.bits) / 2\n",
    "                qmax = -qmin - 1\n",
    "                scale = self.Quantize_input.max_abs_tr / ((qmax - qmin) / 2)\n",
    "                sq_scale = torch.square(scale)\n",
    "                self.qsin_loss_A = torch.mul(sq_scale, QSin(torch.div(input, scale), self.bits))\n",
    "            \n",
    "            return nn.functional.linear(qinput, qweight, self.bias)\n",
    "        else:\n",
    "            return nn.functional.linear(input, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0a982cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Embedding):\n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx=None, max_norm=None,\n",
    "                 norm_type=2.0, scale_grad_by_freq=False, sparse=False,\n",
    "                 quantization: bool = False, q_bits: int = 8):\n",
    "        super(Embedding, self).__init__(num_embeddings, embedding_dim, padding_idx)\n",
    "\n",
    "        self.quantize = True if quantization else False\n",
    "\n",
    "        if self.quantize:\n",
    "            self.bits = q_bits\n",
    "            self.Quantize_weights = Quant(self.bits)\n",
    "            self.Quantize_input = Quant(self.bits)\n",
    "        else:\n",
    "            self.bits = 'FP'\n",
    "\n",
    "    def init(self, input):\n",
    "        self.inputW = input.shape\n",
    "        \n",
    "    def change_mod(self, value, bits = 8, mode = \"dynamic\"):\n",
    "        self.quantize = value\n",
    "        self.bits = bits\n",
    "        self.Quantize_weights = Quant(bits, mode)\n",
    "        self.Quantize_input = Quant(bits, mode)\n",
    "        \n",
    "    def mode(self):\n",
    "        return self.quantize  \n",
    "\n",
    "    def forward(self, input):\n",
    "            \n",
    "        if self.quantize:\n",
    "            qweight = self.Quantize_weights(self.weight)\n",
    "        \n",
    "            return nn.functional.embedding(input, qweight, self.padding_idx, self.max_norm,\n",
    "                 self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
    "        else:\n",
    "            return nn.functional.embedding(input, self.weight, self.padding_idx, self.max_norm,\n",
    "                 self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1d57841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qsin_W(model, bits = 8):\n",
    "    qmin = -1.0 * (2**bits) / 2\n",
    "    qmax = -qmin - 1\n",
    "    loss = 0\n",
    "    for layer in model.modules():\n",
    "        try:\n",
    "            scale = layer.Quantize_weights.max_abs_tr / ((qmax - qmin) / 2)\n",
    "            sq_scale = torch.square(scale)\n",
    "            QSin_w = QSin(torch.div(layer.weight, scale), bits)\n",
    "            loss = loss + torch.mul(sq_scale, QSin_w)\n",
    "        except:\n",
    "            continue\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c5fb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qsin_A(model):\n",
    "    loss = 0\n",
    "    for layer in model.modules():\n",
    "        try:\n",
    "            loss = loss + layer.qsin_loss_A\n",
    "        except:\n",
    "            continue\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad6d8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_Linear(in_features, out_features, bias, weight):\n",
    "    linear = Linear(in_features, out_features)\n",
    "    linear.bias = bias\n",
    "    linear.weight = weight\n",
    "    return linear\n",
    "\n",
    "def get_custom_Embeding(num_embeddings, embedding_dim, padding_idx, weight):\n",
    "    embedding = Embedding(num_embeddings, embedding_dim, padding_idx)\n",
    "    embedding.weight = weight\n",
    "    return embedding\n",
    "\n",
    "def change_layers(model):\n",
    "    for name, layer in model.named_children():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            setattr(model, name, get_custom_Linear(\n",
    "                                                layer.in_features,\n",
    "                                                layer.out_features,\n",
    "                                                layer.bias,\n",
    "                                                layer.weight\n",
    "            ))\n",
    "            \n",
    "        if isinstance(layer, nn.Embedding):\n",
    "            setattr(model, name, get_custom_Embeding(\n",
    "                                                layer.num_embeddings,\n",
    "                                                layer.embedding_dim,\n",
    "                                                layer.padding_idx,\n",
    "                                                layer.weight\n",
    "            ))\n",
    "        change_layers(getattr(model, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1d410",
   "metadata": {},
   "source": [
    "# glue metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3536a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "\n",
    "import datasets\n",
    "\n",
    "\n",
    "_CITATION = \"\"\"\\\n",
    "@inproceedings{wang2019glue,\n",
    "  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n",
    "  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n",
    "  note={In the Proceedings of ICLR.},\n",
    "  year={2019}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "GLUE, the General Language Understanding Evaluation benchmark\n",
    "(https://gluebenchmark.com/) is a collection of resources for training,\n",
    "evaluating, and analyzing natural language understanding systems.\n",
    "\"\"\"\n",
    "\n",
    "_KWARGS_DESCRIPTION = \"\"\"\n",
    "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
    "Args:\n",
    "    predictions: list of predictions to score.\n",
    "        Each translation should be tokenized into a list of tokens.\n",
    "    references: list of lists of references for each translation.\n",
    "        Each reference should be tokenized into a list of tokens.\n",
    "Returns: depending on the GLUE subset, one or several of:\n",
    "    \"accuracy\": Accuracy\n",
    "    \"f1\": F1 score\n",
    "    \"pearson\": Pearson Correlation\n",
    "    \"spearmanr\": Spearman Correlation\n",
    "    \"matthews_correlation\": Matthew Correlation\n",
    "Examples:\n",
    "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
    "    >>> references = [0, 1]\n",
    "    >>> predictions = [0, 1]\n",
    "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
    "    >>> print(results)\n",
    "    {'accuracy': 1.0}\n",
    "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
    "    >>> references = [0, 1]\n",
    "    >>> predictions = [0, 1]\n",
    "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
    "    >>> print(results)\n",
    "    {'accuracy': 1.0, 'f1': 1.0}\n",
    "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
    "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
    "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
    "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
    "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
    "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
    "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
    "    >>> references = [0, 1]\n",
    "    >>> predictions = [0, 1]\n",
    "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
    "    >>> print(results)\n",
    "    {'matthews_correlation': 1.0}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "def pearson_and_spearman(preds, labels):\n",
    "    pearson_corr = pearsonr(preds, labels)[0]\n",
    "    spearman_corr = spearmanr(preds, labels)[0]\n",
    "    return {\n",
    "        \"pearson\": pearson_corr,\n",
    "        \"spearmanr\": spearman_corr,\n",
    "    }\n",
    "\n",
    "\n",
    "@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\n",
    "class Glue(datasets.Metric):\n",
    "    def _info(self):\n",
    "        if self.config_name not in [\n",
    "            \"sst2\",\n",
    "            \"mnli\",\n",
    "            \"mnli_mismatched\",\n",
    "            \"mnli_matched\",\n",
    "            \"cola\",\n",
    "            \"stsb\",\n",
    "            \"mrpc\",\n",
    "            \"qqp\",\n",
    "            \"qnli\",\n",
    "            \"rte\",\n",
    "            \"wnli\",\n",
    "            \"hans\",\n",
    "        ]:\n",
    "            raise KeyError(\n",
    "                \"You should supply a configuration name selected in \"\n",
    "                '[\"sst2\", \"mnli\", \"mnli_mismatched\", \"mnli_matched\", '\n",
    "                '\"cola\", \"stsb\", \"mrpc\", \"qqp\", \"qnli\", \"rte\", \"wnli\", \"hans\"]'\n",
    "            )\n",
    "        return datasets.MetricInfo(\n",
    "            description=_DESCRIPTION,\n",
    "            citation=_CITATION,\n",
    "            inputs_description=_KWARGS_DESCRIPTION,\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"predictions\": datasets.Value(\"int64\" if self.config_name != \"stsb\" else \"float32\"),\n",
    "                    \"references\": datasets.Value(\"int64\" if self.config_name != \"stsb\" else \"float32\"),\n",
    "                }\n",
    "            ),\n",
    "            codebase_urls=[],\n",
    "            reference_urls=[],\n",
    "            format=\"numpy\",\n",
    "        )\n",
    "\n",
    "    def _compute(self, predictions, references):\n",
    "        if self.config_name == \"cola\":\n",
    "            return {\"matthews_correlation\": matthews_corrcoef(references, predictions)}\n",
    "        elif self.config_name == \"stsb\":\n",
    "            return pearson_and_spearman(predictions, references)\n",
    "        elif self.config_name in [\"mrpc\", \"qqp\"]:\n",
    "            return acc_and_f1(predictions, references)\n",
    "        elif self.config_name in [\"sst2\", \"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]:\n",
    "            return {\"accuracy\": simple_accuracy(predictions, references)}\n",
    "        else:\n",
    "            raise KeyError(\n",
    "                \"You should supply a configuration name selected in \"\n",
    "                '[\"sst2\", \"mnli\", \"mnli_mismatched\", \"mnli_matched\", '\n",
    "                '\"cola\", \"stsb\", \"mrpc\", \"qqp\", \"qnli\", \"rte\", \"wnli\", \"hans\"]'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e23be37",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1bb0d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./models/model-bert-base/', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fe188e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import DistilBertTokenizer\n",
    "    \n",
    "batch_size = 32\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('./models/tokenizer-bert-base/', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f831ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "encoded_dataset = load_from_disk('cur_glue_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bbac43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "task = 'stsb'\n",
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "metric = metric = Glue(task)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51fe72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6a9d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c982006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"test-glue\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "344b372f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 02:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearmanr</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.569310</td>\n",
       "      <td>0.877687</td>\n",
       "      <td>0.880065</td>\n",
       "      <td>2.263100</td>\n",
       "      <td>662.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.863400</td>\n",
       "      <td>0.481069</td>\n",
       "      <td>0.891091</td>\n",
       "      <td>0.887412</td>\n",
       "      <td>2.183900</td>\n",
       "      <td>686.856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.483172</td>\n",
       "      <td>0.893608</td>\n",
       "      <td>0.890437</td>\n",
       "      <td>2.196400</td>\n",
       "      <td>682.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1080, training_loss=0.567343967932242, metrics={'train_runtime': 154.8641, 'train_samples_per_second': 6.974, 'total_flos': 1393513314445116.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbe505c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2a60d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoModelForSequenceClassification\n",
    "#model = AutoModelForSequenceClassification.from_pretrained('./test-glue/checkpoint-720/',\n",
    "#                                                           local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23f29eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "856d58b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4810692071914673,\n",
       " 'eval_pearson': 0.8910907832498873,\n",
       " 'eval_spearmanr': 0.8874122931185459,\n",
       " 'eval_runtime': 2.1953,\n",
       " 'eval_samples_per_second': 683.276}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27326b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantize mode off\n",
      "Quantize mode on\n"
     ]
    }
   ],
   "source": [
    "model = quantize_model(model, quantize=False, bits = 8)\n",
    "model = quantize_model(model, quantize=True, bits = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2fb0b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eval_DQ = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ace3b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.370012640953064,\n",
       " 'eval_pearson': 0.8496805226953267,\n",
       " 'eval_spearmanr': 0.8628488003329071,\n",
       " 'eval_runtime': 3.4085,\n",
       " 'eval_samples_per_second': 440.072}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eval_DQ.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25c056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43c1a166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantize mode off\n",
      "Quantize mode on\n"
     ]
    }
   ],
   "source": [
    "model = quantize_model(model, quantize=False, bits = 8)\n",
    "model = quantize_model(model, quantize=True, bits = 8, qmode = \"static\")\n",
    "encoded_dataset_static = load_from_disk('cur_glue_data_st')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2bce85d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc = encoded_dataset_static['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aeb60892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_enc.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "train_loader = torch.utils.data.DataLoader(train_enc, batch_size=8, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee4befbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 31/719 [00:24<09:04,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "i = 0\n",
    "for batch in tqdm(train_loader):\n",
    "    i += 1\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['label'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    if i == 32:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eebebef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"test-glue\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    metric_for_best_model=metric_name,\n",
    ")\n",
    "\n",
    "Eval_SQ = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e897121b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47/47 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.4507509469985962,\n",
       " 'eval_pearson': 0.8559594621511081,\n",
       " 'eval_spearmanr': 0.8663923035421431,\n",
       " 'eval_runtime': 2.7595,\n",
       " 'eval_samples_per_second': 543.579}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eval_SQ.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d0f91a",
   "metadata": {},
   "source": [
    "# QSIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2bafd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QSIN activation mode on\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qsin_activation_mode(model)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa96398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7464a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "class QSinTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        Qsin_W_loss = Qsin_W(model, 8)\n",
    "        Qsin_A_loss = Qsin_A(model)\n",
    "        L = outputs[0]\n",
    "        lambda_w = 10 ** (np.round(np.log10(Qsin_W_loss.cuda().tolist()) - np.log10(L.cuda().tolist())))\n",
    "        lambda_a = 10 ** (np.round(np.log10(Qsin_A_loss.cuda().tolist()) - np.log10(L.cuda().tolist()))+1)\n",
    "        loss = L + Qsin_W_loss / lambda_w + Qsin_A_loss / lambda_a\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2651ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"qsin_train_tmp\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    #eval_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")\n",
    "\n",
    "trainer_QSin = QSinTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ace42e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [540/540 12:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearmanr</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.298086</td>\n",
       "      <td>0.890405</td>\n",
       "      <td>0.886263</td>\n",
       "      <td>8.809100</td>\n",
       "      <td>170.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.176635</td>\n",
       "      <td>0.893213</td>\n",
       "      <td>0.889401</td>\n",
       "      <td>8.868300</td>\n",
       "      <td>169.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.491100</td>\n",
       "      <td>1.135844</td>\n",
       "      <td>0.895877</td>\n",
       "      <td>0.892030</td>\n",
       "      <td>8.877200</td>\n",
       "      <td>168.972000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at qsin_train_tmp/checkpoint-540 were not used when initializing BertForSequenceClassification: ['bert.embeddings.word_embeddings.Quantize_weights.max_abs_tr', 'bert.embeddings.word_embeddings.Quantize_input.max_abs_tr', 'bert.embeddings.position_embeddings.Quantize_weights.max_abs_tr', 'bert.embeddings.position_embeddings.Quantize_input.max_abs_tr', 'bert.embeddings.token_type_embeddings.Quantize_weights.max_abs_tr', 'bert.embeddings.token_type_embeddings.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.output.dense.Quantize_input.max_abs_tr', 'bert.pooler.dense.Quantize_weights.max_abs_tr', 'bert.pooler.dense.Quantize_input.max_abs_tr', 'classifier.Quantize_weights.max_abs_tr', 'classifier.Quantize_input.max_abs_tr']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=540, training_loss=0.47881572864673755, metrics={'train_runtime': 746.3863, 'train_samples_per_second': 0.723, 'total_flos': 1415414277021360.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_QSin.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df7300",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "138d9401",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"qat_train_tmp\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    #eval_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")\n",
    "\n",
    "\n",
    "trainer_QAT = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "238b91d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd44e48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 02:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearmanr</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.620719</td>\n",
       "      <td>0.885621</td>\n",
       "      <td>0.883652</td>\n",
       "      <td>2.119300</td>\n",
       "      <td>707.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.283100</td>\n",
       "      <td>0.449922</td>\n",
       "      <td>0.896620</td>\n",
       "      <td>0.891995</td>\n",
       "      <td>2.121500</td>\n",
       "      <td>707.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>0.443404</td>\n",
       "      <td>0.898980</td>\n",
       "      <td>0.894355</td>\n",
       "      <td>2.121900</td>\n",
       "      <td>706.918000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1080, training_loss=0.21297242994661683, metrics={'train_runtime': 153.142, 'train_samples_per_second': 7.052, 'total_flos': 1393513314445116.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_QAT.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c559199",
   "metadata": {},
   "source": [
    "# QSIN STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be178633",
   "metadata": {},
   "outputs": [],
   "source": [
    "qsin_activation_mode(model)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2492632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "class QSinTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        Qsin_W_loss = Qsin_W(model, 8)\n",
    "        Qsin_A_loss = Qsin_A(model)\n",
    "        L = outputs[0]\n",
    "        lambda_w = 10 ** (np.round(np.log10(Qsin_W_loss.cuda().tolist()) - np.log10(L.cuda().tolist())))\n",
    "        lambda_a = 10 ** (np.round(np.log10(Qsin_A_loss.cuda().tolist()) - np.log10(L.cuda().tolist()))+1)\n",
    "        loss = L + Qsin_W_loss / lambda_w + Qsin_A_loss / lambda_a\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd0dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"qsin_train_tmp\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_steps=5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=metric_name,\n",
    ")\n",
    "\n",
    "trainer_QSin = QSinTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_QSin.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d400914f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
