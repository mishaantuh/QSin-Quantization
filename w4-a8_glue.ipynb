{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.function import InplaceFunction, Function\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, quantize = False, bits = 8, qmode = \"dynamic\"):\n",
    "    if quantize:\n",
    "        print(\"Quantize mode on\")\n",
    "        for layer in model.modules():\n",
    "            try:\n",
    "                mode = layer.mode()\n",
    "                if mode == False:\n",
    "                    layer.change_mod(True, bits, qmode)\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        print(\"Quantize mode off\")\n",
    "        for layer in model.modules():\n",
    "            try:\n",
    "                mode = layer.mode()\n",
    "                if mode == True:\n",
    "                    layer.change_mod(False, 0)\n",
    "            except:\n",
    "                continue\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qsin_activation_mode(model):\n",
    "    print(\"QSIN activation mode on\")\n",
    "    for layer in model.modules():\n",
    "        try:\n",
    "            mode = layer.mode()\n",
    "            layer.qsinmode()\n",
    "        except:\n",
    "            continue\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRound(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quantize_tensor(input_tensor, max_abs_val = None, num_bits = 8):\n",
    "    my_round = MyRound.apply\n",
    "    qmin = -1.0 * (2**num_bits) / 2\n",
    "    qmax = -qmin - 1\n",
    "    scale = max_abs_val / ((qmax - qmin) / 2)\n",
    "    input_tensor = torch.div(input_tensor, scale)\n",
    "    input_tensor = my_round((input_tensor))\n",
    "    input_tensor = torch.clamp(input_tensor, qmin, qmax)\n",
    "    return torch.mul(input_tensor, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quant(nn.Module):\n",
    "    def __init__(self, num_bits=8, mode = \"dynamic\", static_count = 30):\n",
    "        super(Quant, self).__init__()\n",
    "        self.num_bits = num_bits\n",
    "        self.mode = mode\n",
    "        self.static_count = static_count\n",
    "        self.static_cur = 0\n",
    "        self.stat_values = []\n",
    "        self.max_abs = 0 \n",
    "        if mode != \"dynamic\":\n",
    "            self.max_abs_tr = nn.Parameter(torch.zeros(0), requires_grad=True) # IMPORTANT\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if self.mode == \"dynamic\":\n",
    "            self.max_abs = torch.max(torch.abs(input.detach()))\n",
    "            return Quantize_tensor(input, self.max_abs, self.num_bits)\n",
    "        \n",
    "        elif self.mode == \"static\":\n",
    "            if self.static_cur > self.static_count:\n",
    "                return Quantize_tensor(input, self.max_abs_tr, self.num_bits)\n",
    "            elif self.static_cur == self.static_count:\n",
    "                self.max_abs = np.mean(self.stat_values)\n",
    "                self.max_abs_tr.data = torch.tensor(self.max_abs, dtype=torch.float).to(self.max_abs_tr.device)\n",
    "                self.static_cur += 1\n",
    "                return Quantize_tensor(input, self.max_abs_tr, self.num_bits)\n",
    "            else:\n",
    "                self.static_cur += 1\n",
    "                self.stat_values.append(np.max(np.absolute(input.cpu().detach().numpy())))\n",
    "                return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QSin(x, num_bits = 8): \n",
    "    pi = torch.tensor(np.pi)\n",
    "    qmin = -1.0 * (2**num_bits) / 2\n",
    "    qmax = -qmin - 1\n",
    "    result = torch.sum(torch.square(torch.sin(torch.mul(pi, x[torch.logical_and(x >= qmin, x <= qmax)]))))\n",
    "    result = result + torch.sum(torch.mul(torch.square(pi), torch.square((x[x < qmin] - qmin))))\n",
    "    result = result + torch.sum(torch.mul(torch.square(pi), torch.square((x[x > qmax] - qmax))))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, quantization: bool = False, q_bits: int = 8, qsin_activation = False):\n",
    "        super(Linear, self).__init__(in_features, out_features, bias)\n",
    "\n",
    "        self.quantize = True if quantization else False\n",
    "        self.QsinA = True if qsin_activation else False\n",
    "\n",
    "        if self.quantize:\n",
    "            self.bits = q_bits\n",
    "            self.Quantize_weights = Quant(self.bits)\n",
    "            self.Quantize_input = Quant(8) \n",
    "        else:\n",
    "            self.bits = 'FP'\n",
    "            \n",
    "        if self.QsinA:\n",
    "            self.qsin_loss_A = 0\n",
    "\n",
    "    def init(self, input):\n",
    "        self.inputW = input.shape\n",
    "        \n",
    "    def change_mod(self, value, bits = 8, mode = \"dynamic\"):\n",
    "        self.quantize = value\n",
    "        self.bits = bits\n",
    "        self.Quantize_weights = Quant(bits, mode)\n",
    "        self.Quantize_input = Quant(8, mode)\n",
    "        \n",
    "    def qsinmode(self):\n",
    "        self.QsinA = True\n",
    "        self.qsin_loss_A = 0\n",
    "        \n",
    "    def mode(self):\n",
    "        return self.quantize  \n",
    "\n",
    "    def forward(self, input):\n",
    "            \n",
    "        if self.quantize:\n",
    "            qinput = self.Quantize_input(input)\n",
    "            qweight = self.Quantize_weights(self.weight)\n",
    "            \n",
    "            #count qsin loss on activation\n",
    "            if self.QsinA:\n",
    "                self.qsin_loss_A = 0\n",
    "                qmin = -1.0 * (2**8) / 2\n",
    "                qmax = -qmin - 1\n",
    "                scale = self.Quantize_input.max_abs_tr / ((qmax - qmin) / 2)\n",
    "                sq_scale = torch.square(scale)\n",
    "                self.qsin_loss_A = torch.mul(sq_scale, QSin(torch.div(input, scale), 8))\n",
    "            \n",
    "            return nn.functional.linear(qinput, qweight, self.bias)\n",
    "        else:\n",
    "            return nn.functional.linear(input, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Embedding):\n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx=None, max_norm=None,\n",
    "                 norm_type=2.0, scale_grad_by_freq=False, sparse=False,\n",
    "                 quantization: bool = False, q_bits: int = 8):\n",
    "        super(Embedding, self).__init__(num_embeddings, embedding_dim, padding_idx)\n",
    "\n",
    "        self.quantize = True if quantization else False\n",
    "\n",
    "        if self.quantize:\n",
    "            self.bits = q_bits\n",
    "            self.Quantize_weights = Quant(self.bits)\n",
    "        else:\n",
    "            self.bits = 'FP'\n",
    "\n",
    "    def init(self, input):\n",
    "        self.inputW = input.shape\n",
    "        \n",
    "    def change_mod(self, value, bits = 8, mode = \"dynamic\"):\n",
    "        self.quantize = value\n",
    "        self.bits = bits\n",
    "        self.Quantize_weights = Quant(bits, mode)\n",
    "        \n",
    "    def mode(self):\n",
    "        return self.quantize  \n",
    "\n",
    "    def forward(self, input):\n",
    "            \n",
    "        if self.quantize:\n",
    "            qweight = self.Quantize_weights(self.weight)\n",
    "        \n",
    "            return nn.functional.embedding(input, qweight, self.padding_idx, self.max_norm,\n",
    "                 self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
    "        else:\n",
    "            return nn.functional.embedding(input, self.weight, self.padding_idx, self.max_norm,\n",
    "                 self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qsin_W(model, bits = 8):\n",
    "    qmin = -1.0 * (2**bits) / 2\n",
    "    qmax = -qmin - 1\n",
    "    loss = 0\n",
    "    for layer in model.modules():\n",
    "        try:\n",
    "            scale = layer.Quantize_weights.max_abs_tr / ((qmax - qmin) / 2)\n",
    "            sq_scale = torch.square(scale)\n",
    "            QSin_w = QSin(torch.div(layer.weight, scale), bits)\n",
    "            loss = loss + torch.mul(sq_scale, QSin_w)\n",
    "        except:\n",
    "            continue\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qsin_A(model):\n",
    "    loss = 0\n",
    "    for layer in model.modules():\n",
    "        try:\n",
    "            loss = loss + layer.qsin_loss_A\n",
    "        except:\n",
    "            continue\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_Linear(in_features, out_features, bias, weight):\n",
    "    linear = Linear(in_features, out_features)\n",
    "    linear.bias = bias\n",
    "    linear.weight = weight\n",
    "    return linear\n",
    "\n",
    "def get_custom_Embeding(num_embeddings, embedding_dim, padding_idx, weight):\n",
    "    embedding = Embedding(num_embeddings, embedding_dim, padding_idx)\n",
    "    embedding.weight = weight\n",
    "    return embedding\n",
    "\n",
    "def change_layers(model):\n",
    "    for name, layer in model.named_children():\n",
    "        #if name == 'intermediate' or \\\n",
    "        #name == 'output'or name == 'embeddings':\n",
    "        #   continue\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            setattr(model, name, get_custom_Linear(\n",
    "                                                layer.in_features,\n",
    "                                                layer.out_features,\n",
    "                                                layer.bias,\n",
    "                                                layer.weight\n",
    "            ))\n",
    "            \n",
    "        if isinstance(layer, nn.Embedding):\n",
    "            setattr(model, name, get_custom_Embeding(\n",
    "                                                layer.num_embeddings,\n",
    "                                                layer.embedding_dim,\n",
    "                                                layer.padding_idx,\n",
    "                                                layer.weight\n",
    "            ))\n",
    "        change_layers(getattr(model, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pooler\n",
    "#classifier\n",
    "#attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, quantize = False, bits = 8, qmode = \"dynamic\"):\n",
    "    if quantize:\n",
    "        print(\"Quantize mode on\")\n",
    "        for name, layer in model.named_modules():\n",
    "            try:\n",
    "                mode = layer.mode()\n",
    "                if mode == False:\n",
    "                    if 'pooler' in name or 'attention' in name or 'token_type_embeddings' in name:\n",
    "                        layer.change_mod(True, 4, qmode)\n",
    "                    elif 'classifier' in name: continue \n",
    "                    else: layer.change_mod(True, 8, qmode)\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        print(\"Quantize mode off\")\n",
    "        for layer in model.modules():\n",
    "            try:\n",
    "                mode = layer.mode()\n",
    "                if mode == True:\n",
    "                    layer.change_mod(False, 0)\n",
    "            except:\n",
    "                continue\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pooler\n",
    "#classifier\n",
    "#attention\n",
    "\n",
    "        #if name == 'intermediate' or \\\n",
    "        #name == 'output'or name == 'embeddings':\n",
    "        #    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glue metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "\n",
    "import datasets\n",
    "\n",
    "\n",
    "_CITATION = \"\"\"\\\n",
    "@inproceedings{wang2019glue,\n",
    "  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n",
    "  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n",
    "  note={In the Proceedings of ICLR.},\n",
    "  year={2019}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "GLUE, the General Language Understanding Evaluation benchmark\n",
    "(https://gluebenchmark.com/) is a collection of resources for training,\n",
    "evaluating, and analyzing natural language understanding systems.\n",
    "\"\"\"\n",
    "\n",
    "_KWARGS_DESCRIPTION = \"\"\"\n",
    "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
    "Args:\n",
    "    predictions: list of predictions to score.\n",
    "        Each translation should be tokenized into a list of tokens.\n",
    "    references: list of lists of references for each translation.\n",
    "        Each reference should be tokenized into a list of tokens.\n",
    "Returns: depending on the GLUE subset, one or several of:\n",
    "    \"accuracy\": Accuracy\n",
    "    \"f1\": F1 score\n",
    "    \"pearson\": Pearson Correlation\n",
    "    \"spearmanr\": Spearman Correlation\n",
    "    \"matthews_correlation\": Matthew Correlation\n",
    "Examples:\n",
    "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
    "    >>> references = [0, 1]\n",
    "    >>> predictions = [0, 1]\n",
    "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
    "    >>> print(results)\n",
    "    {'accuracy': 1.0}\n",
    "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
    "    >>> references = [0, 1]\n",
    "    >>> predictions = [0, 1]\n",
    "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
    "    >>> print(results)\n",
    "    {'accuracy': 1.0, 'f1': 1.0}\n",
    "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
    "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
    "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
    "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
    "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
    "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
    "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
    "    >>> references = [0, 1]\n",
    "    >>> predictions = [0, 1]\n",
    "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
    "    >>> print(results)\n",
    "    {'matthews_correlation': 1.0}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "def pearson_and_spearman(preds, labels):\n",
    "    pearson_corr = pearsonr(preds, labels)[0]\n",
    "    spearman_corr = spearmanr(preds, labels)[0]\n",
    "    return {\n",
    "        \"pearson\": pearson_corr,\n",
    "        \"spearmanr\": spearman_corr,\n",
    "    }\n",
    "\n",
    "\n",
    "@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\n",
    "class Glue(datasets.Metric):\n",
    "    def _info(self):\n",
    "        if self.config_name not in [\n",
    "            \"sst2\",\n",
    "            \"mnli\",\n",
    "            \"mnli_mismatched\",\n",
    "            \"mnli_matched\",\n",
    "            \"cola\",\n",
    "            \"stsb\",\n",
    "            \"mrpc\",\n",
    "            \"qqp\",\n",
    "            \"qnli\",\n",
    "            \"rte\",\n",
    "            \"wnli\",\n",
    "            \"hans\",\n",
    "        ]:\n",
    "            raise KeyError(\n",
    "                \"You should supply a configuration name selected in \"\n",
    "                '[\"sst2\", \"mnli\", \"mnli_mismatched\", \"mnli_matched\", '\n",
    "                '\"cola\", \"stsb\", \"mrpc\", \"qqp\", \"qnli\", \"rte\", \"wnli\", \"hans\"]'\n",
    "            )\n",
    "        return datasets.MetricInfo(\n",
    "            description=_DESCRIPTION,\n",
    "            citation=_CITATION,\n",
    "            inputs_description=_KWARGS_DESCRIPTION,\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"predictions\": datasets.Value(\"int64\" if self.config_name != \"stsb\" else \"float32\"),\n",
    "                    \"references\": datasets.Value(\"int64\" if self.config_name != \"stsb\" else \"float32\"),\n",
    "                }\n",
    "            ),\n",
    "            codebase_urls=[],\n",
    "            reference_urls=[],\n",
    "            format=\"numpy\",\n",
    "        )\n",
    "\n",
    "    def _compute(self, predictions, references):\n",
    "        if self.config_name == \"cola\":\n",
    "            return {\"matthews_correlation\": matthews_corrcoef(references, predictions)}\n",
    "        elif self.config_name == \"stsb\":\n",
    "            return pearson_and_spearman(predictions, references)\n",
    "        elif self.config_name in [\"mrpc\", \"qqp\"]:\n",
    "            return acc_and_f1(predictions, references)\n",
    "        elif self.config_name in [\"sst2\", \"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]:\n",
    "            return {\"accuracy\": simple_accuracy(predictions, references)}\n",
    "        else:\n",
    "            raise KeyError(\n",
    "                \"You should supply a configuration name selected in \"\n",
    "                '[\"sst2\", \"mnli\", \"mnli_mismatched\", \"mnli_matched\", '\n",
    "                '\"cola\", \"stsb\", \"mrpc\", \"qqp\", \"qnli\", \"rte\", \"wnli\", \"hans\"]'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./models/model-bert-base/', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import DistilBertTokenizer\n",
    "    \n",
    "batch_size = 16\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('./models/tokenizer-bert-base/', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "encoded_dataset = load_from_disk('cur_glue_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "task = 'cola'\n",
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "metric = metric = Glue(task)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437.93716\n"
     ]
    }
   ],
   "source": [
    "num_parameters = sum(p.numel() for name, p in model.state_dict().items())\n",
    "print(num_parameters * 32 / 8 * 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position_ids : 512\n",
      "word_embeddings.weight : 23440896\n",
      "position_embeddings.weight : 393216\n",
      "token_type_embeddings.weight : 1536\n",
      "LayerNorm.weight : 768\n",
      "LayerNorm.bias : 768\n",
      "95.35078399999999\n"
     ]
    }
   ],
   "source": [
    "w = 0\n",
    "for name, p in model.bert.embeddings.state_dict().items():\n",
    "    w += p.numel()\n",
    "    print(name, \":\", p.numel())\n",
    "print(w * 32 / 8 * 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention.self.query.weight : 589824\n",
      "attention.self.query.bias : 768\n",
      "attention.self.key.weight : 589824\n",
      "attention.self.key.bias : 768\n",
      "attention.self.value.weight : 589824\n",
      "attention.self.value.bias : 768\n",
      "attention.output.dense.weight : 589824\n",
      "attention.output.dense.bias : 768\n",
      "attention.output.LayerNorm.weight : 768\n",
      "attention.output.LayerNorm.bias : 768\n",
      "intermediate.dense.weight : 2359296\n",
      "intermediate.dense.bias : 3072\n",
      "output.dense.weight : 2359296\n",
      "output.dense.bias : 768\n",
      "output.LayerNorm.weight : 768\n",
      "output.LayerNorm.bias : 768\n",
      "340.217856\n"
     ]
    }
   ],
   "source": [
    "w = 0\n",
    "for name, p in model.bert.encoder.layer[0].state_dict().items():\n",
    "    w += p.numel()\n",
    "    print(name, \":\", p.numel())\n",
    "print(w * 12 * 32 / 8 * 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : 1536\n",
      "bias : 2\n",
      "0.0061519999999999995\n"
     ]
    }
   ],
   "source": [
    "w = 0\n",
    "for name, p in model.classifier.state_dict().items():\n",
    "    w += p.numel()\n",
    "    print(name, \":\", p.numel())\n",
    "print(w * 32 / 8 * 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense.weight : 589824\n",
      "dense.bias : 768\n",
      "2.362368\n"
     ]
    }
   ],
   "source": [
    "w = 0\n",
    "for name, p in model.bert.pooler.state_dict().items():\n",
    "    w += p.numel()\n",
    "    print(name, \":\", p.numel())\n",
    "print(w * 32 / 8 * 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.843072\n"
     ]
    }
   ],
   "source": [
    "position_ids = 512\n",
    "word_embeddings_weight = 23440896\n",
    "position_embeddings_weight = 393216\n",
    "token_type_embeddings_weight = 1536\n",
    "LayerNorm_weight = 768\n",
    "LayerNorm_bias = 768\n",
    "\n",
    "embd = position_ids * 32 + word_embeddings_weight * 8 \n",
    "embd += position_embeddings_weight * 8 + token_type_embeddings_weight * 4\n",
    "embd += LayerNorm_weight * 32 + LayerNorm_bias * 32 \n",
    "print(embd / 8 * 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.14752\n"
     ]
    }
   ],
   "source": [
    "attention_self_query_weight = 589824\n",
    "intermediate_dense_weight = 2359296\n",
    "output_dense_weight = 2359296\n",
    "    \n",
    "att = attention_self_query_weight * 4 * 4 # bits * count\n",
    "att += intermediate_dense_weight * 8\n",
    "att += output_dense_weight * 8\n",
    "att += 768 * 32 * 6 + 3072 * 32\n",
    "print(att / 8 * 1e-6 * 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29798399999999997\n"
     ]
    }
   ],
   "source": [
    "# pooler\n",
    "dense_weight = 589824\n",
    "dense_bias = 768\n",
    "pooler = dense_weight * 4 + dense_bias * 32\n",
    "print(pooler / 8 * 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437.82041599999997"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all 32 bits\n",
    "95.35078399999999 + 340.107264 + 2.362368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109.74003199999999"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all 8 bits sheme\n",
    "23.84384 + 85.30329599999999 + 0.592896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.28857599999999"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mix 4-8 bits\n",
    "23.843072 + 71.14752 + 0.29798399999999997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9896144371454167"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "437.82041599999997 / 109.74003199999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.59467896760258"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "437.82041599999997 / 95.28857599999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"test-glue\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1072' max='1072' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1072/1072 02:06, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.459633</td>\n",
       "      <td>0.485139</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>1426.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.438300</td>\n",
       "      <td>0.425990</td>\n",
       "      <td>0.531160</td>\n",
       "      <td>0.674100</td>\n",
       "      <td>1547.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.438300</td>\n",
       "      <td>0.467304</td>\n",
       "      <td>0.552074</td>\n",
       "      <td>0.674400</td>\n",
       "      <td>1546.664000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.205400</td>\n",
       "      <td>0.512932</td>\n",
       "      <td>0.573205</td>\n",
       "      <td>0.675200</td>\n",
       "      <td>1544.764000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1072, training_loss=0.3110503678891196, metrics={'train_runtime': 126.4256, 'train_samples_per_second': 8.479, 'total_flos': 1051111929774804.0, 'epoch': 4.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./test-glue/checkpoint-1072/',\n",
    "                                                           local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5129320621490479,\n",
       " 'eval_matthews_correlation': 0.5732046470010711,\n",
       " 'eval_runtime': 0.7093,\n",
       " 'eval_samples_per_second': 1470.519}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantize mode off\n",
      "Quantize mode on\n"
     ]
    }
   ],
   "source": [
    "model = quantize_model(model, quantize=False, bits = 4)\n",
    "model = quantize_model(model, quantize=True, bits = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eval_DQ = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4798387289047241,\n",
       " 'eval_matthews_correlation': 0.41682112176346214,\n",
       " 'eval_runtime': 1.073,\n",
       " 'eval_samples_per_second': 972.048}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eval_DQ.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantize mode off\n",
      "Quantize mode on\n"
     ]
    }
   ],
   "source": [
    "model = quantize_model(model, quantize=False, bits = 4)\n",
    "model = quantize_model(model, quantize=True, bits = 4, qmode = \"static\")\n",
    "encoded_dataset_static = load_from_disk('cur_glue_data_st')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc = encoded_dataset_static['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_enc.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "train_loader = torch.utils.data.DataLoader(train_enc, batch_size=8, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 31/1069 [00:33<18:30,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "i = 0\n",
    "for batch in tqdm(train_loader):\n",
    "    i += 1\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['label'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    if i == 32:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5073927044868469,\n",
       " 'eval_matthews_correlation': 0.34197644445477027,\n",
       " 'eval_runtime': 1.3166,\n",
       " 'eval_samples_per_second': 792.194}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    \"test-glue\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    metric_for_best_model=metric_name,\n",
    ")\n",
    "\n",
    "Eval_SQ = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "Eval_SQ.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QSIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qsin_activation_mode(model)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "class QSinTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        Qsin_W_loss = Qsin_W(model, 4)\n",
    "        Qsin_A_loss = Qsin_A(model)\n",
    "        L = outputs[0]\n",
    "        lambda_w = 10 ** (np.round(np.log10(Qsin_W_loss.cuda().tolist()) - np.log10(L.cuda().tolist())))\n",
    "        lambda_a = 10 ** (np.round(np.log10(Qsin_A_loss.cuda().tolist()) - np.log10(L.cuda().tolist()))+1)\n",
    "        loss = L + Qsin_W_loss / lambda_w + Qsin_A_loss / lambda_a\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"qsin_train_tmp\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    #eval_steps=10, \n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")\n",
    "\n",
    "trainer_QSin = QSinTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2160' max='2160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2160/2160 32:19, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearmanr</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.338755</td>\n",
       "      <td>0.874005</td>\n",
       "      <td>0.872234</td>\n",
       "      <td>12.667800</td>\n",
       "      <td>118.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.252400</td>\n",
       "      <td>1.177294</td>\n",
       "      <td>0.881382</td>\n",
       "      <td>0.879060</td>\n",
       "      <td>12.663100</td>\n",
       "      <td>118.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>1.130876</td>\n",
       "      <td>0.884721</td>\n",
       "      <td>0.881053</td>\n",
       "      <td>12.646400</td>\n",
       "      <td>118.611000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>1.188228</td>\n",
       "      <td>0.886590</td>\n",
       "      <td>0.882593</td>\n",
       "      <td>12.625900</td>\n",
       "      <td>118.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.482500</td>\n",
       "      <td>1.154018</td>\n",
       "      <td>0.886627</td>\n",
       "      <td>0.882877</td>\n",
       "      <td>12.622400</td>\n",
       "      <td>118.836000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>1.118148</td>\n",
       "      <td>0.890415</td>\n",
       "      <td>0.886659</td>\n",
       "      <td>12.681700</td>\n",
       "      <td>118.280000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at qsin_train_tmp/checkpoint-2160 were not used when initializing BertForSequenceClassification: ['bert.embeddings.word_embeddings.Quantize_weights.max_abs_tr', 'bert.embeddings.position_embeddings.Quantize_weights.max_abs_tr', 'bert.embeddings.token_type_embeddings.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.output.dense.Quantize_input.max_abs_tr', 'bert.pooler.dense.Quantize_weights.max_abs_tr', 'bert.pooler.dense.Quantize_input.max_abs_tr']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2160, training_loss=0.6629010659677012, metrics={'train_runtime': 1940.7698, 'train_samples_per_second': 1.113, 'total_flos': 2788098539560632.0, 'epoch': 6.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_QSin.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"qat_train_tmp\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    #eval_steps=10,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")\n",
    "\n",
    "\n",
    "trainer_QAT = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2160' max='2160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2160/2160 07:35, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearmanr</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.564630</td>\n",
       "      <td>0.879557</td>\n",
       "      <td>0.874765</td>\n",
       "      <td>2.981000</td>\n",
       "      <td>503.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>0.539674</td>\n",
       "      <td>0.879536</td>\n",
       "      <td>0.876675</td>\n",
       "      <td>2.984500</td>\n",
       "      <td>502.593000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.262800</td>\n",
       "      <td>0.486788</td>\n",
       "      <td>0.887080</td>\n",
       "      <td>0.881744</td>\n",
       "      <td>2.979900</td>\n",
       "      <td>503.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.262800</td>\n",
       "      <td>0.481755</td>\n",
       "      <td>0.886697</td>\n",
       "      <td>0.883014</td>\n",
       "      <td>2.982500</td>\n",
       "      <td>502.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.150100</td>\n",
       "      <td>0.464847</td>\n",
       "      <td>0.890952</td>\n",
       "      <td>0.887180</td>\n",
       "      <td>2.984800</td>\n",
       "      <td>502.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>0.472170</td>\n",
       "      <td>0.890047</td>\n",
       "      <td>0.885800</td>\n",
       "      <td>2.981700</td>\n",
       "      <td>503.077000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at qat_train_tmp/checkpoint-1800 were not used when initializing BertForSequenceClassification: ['bert.embeddings.word_embeddings.Quantize_weights.max_abs_tr', 'bert.embeddings.position_embeddings.Quantize_weights.max_abs_tr', 'bert.embeddings.token_type_embeddings.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.0.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.0.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.1.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.1.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.2.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.2.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.3.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.3.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.4.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.4.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.5.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.5.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.6.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.6.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.7.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.7.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.8.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.8.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.9.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.9.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.10.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.10.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.attention.self.query.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.attention.self.query.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.attention.self.key.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.attention.self.key.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.attention.self.value.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.attention.self.value.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.attention.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.attention.output.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.intermediate.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.intermediate.dense.Quantize_input.max_abs_tr', 'bert.encoder.layer.11.output.dense.Quantize_weights.max_abs_tr', 'bert.encoder.layer.11.output.dense.Quantize_input.max_abs_tr', 'bert.pooler.dense.Quantize_weights.max_abs_tr', 'bert.pooler.dense.Quantize_input.max_abs_tr']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2160, training_loss=0.24185576218145866, metrics={'train_runtime': 456.1213, 'train_samples_per_second': 4.736, 'total_flos': 2788098539560632.0, 'epoch': 6.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_QAT.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
